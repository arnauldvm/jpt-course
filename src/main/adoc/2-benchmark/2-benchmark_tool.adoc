// build_options: 
Java Performance Tuning - Benchmarking tools
============================================
Arnauld Van Muysewinkel <avm@pendragon.be>
v0.1, 25-Oct-2015: Draft version
:backend: slidy
//:theme: volnitsky
:data-uri:
:copyright: Creative-Commons-Zero (Arnauld Van Muysewinkel)
:icons:
:br: pass:[<br>]
:nil: pass:[</>]


Content
-------

* <<_rtri_requirements,Requirements>>
// GUI (heavy client) testing (p38-41)
// Regression testing (p42)
* <<_rtri_market_overview,Market overview>>
// injectors (p37)
// Standard APi testing (p43-44) + Gattling, LoadRunner ...
* <<_rtri_jmeter,JMeter>>
// Jmeter ... (pp45...83, 89...97)
** Structure
** Lifecycle
** Components
// Threading model (p84-89)
* Mock-Ups
// (p98-99)
* Practical considerations
** Randomization
// ! _random_ parameters
// ! randomize _before_ run
// randomize timers
** duration
// statistical representativeness
** caching!
// p27-28
** https://regex101.com
* analysing/graphing/reporting...

_(link:../0-extra/1-training_plan.html#_presentations[back to course plan])_


&rtri; Requirements
-------------------

An injector is a tool that is able to _reproducibly_ generate a _well-defined_ load.

* Submit (receive) any kind of HTTP request (response)
* Submit other kind of requests (JDBC, RMI, (SOAP), execute Java code for specific cases)
* Represent complex scenario logic 
* Management of high number of virtual users
* _'Parameterizable'_
* Recording of results

Optional:

* real time display
* analysis, reporting


Kinds of testing tools
----------------------

[style="asciidoc",cols="^10,^1,^10,^1,^10",grid="none",frame="none"]
|=====
|GUI testing
|>
|*Protocol level testing*
|>
|Unit testing
|=====

* GUI testing
** fragile
** e.g. FAT client testing
* *Protocol (or API) level testing*
** HTTP, FTP, SOAP, SMTP, JDBC...
** robust, good isolation level -> best for performance testing
** e.g. web based
* Unit testing = Non-regression testing
** closer to micro-benchmarking
** e.g. JUnit


&rtri; Market Overview
----------------------

OpenSource software:

* *JMeter*: extended GUI, many protocols, scenario = XML tree
* Gatling: scenario = Groovy code
* Grinder: distributed testing with many injectors
* ...

Commercial software:

* HP LoadRunner (very complex)
* IBM Rational Performance Tester
* Borland Silk Performer
* ...

Commercial online services (cloud based):

* BlazeMeter
* Blitz
* ...


&rtri; JMeter
-------------

image::jmeter-images/jmeter.jpg[]

* image:jmeter-images/icon-apache.png[] Apache project: https://jmeter.apache.project
* 100% Java
* Many protocols:
** Web - HTTP, HTTPS
** SOAP / REST
** FTP
** Database via JDBC, MongoDB (NoSQL)
** LDAP
** Message-oriented middleware (MOM) via JMS
** Mail - SMTP(S), POP3(S) and IMAP(S)
** Native commands or shell scripts
** TCP
* Many plugins (especially http://jmeter-plugins.org/)


JMeter - Modes
--------------

* GUI -> development, testing of the script
* command-line -> scripted execution
* distributed (servers (slave) + one controller)


JMeter - Components
-------------------

[horizontal]
image:jmeter-images/beaker.gif[height=28] Test Plan:: the whole
image:jmeter-images/testtubes.png[height=28] Configuration Elements:: management of variable parameters
&nbsp; -> Properties:: global scope (shared by all threads)
&nbsp; -> Variables:: thread scope (visible only inside a thread)
image:jmeter-images/thread.gif[height=28] Thread Groups:: virtual users
image:jmeter-images/timer.gif[height=28] Timers:: introduce variable delays -> control of the throughput
image:jmeter-images/knob.gif[height=28] Logic Controllers:: conditional, loop, switch...
image:jmeter-images/pipet.png[height=28] Samplers:: execution of a request for various protocols{br}
  -> This is the _lowest level of granularity for the measures_
image:jmeter-images/leafnode.gif[height=28] image:jmeter-images/leafnodeflip.gif[height=28] Pre/Post-processors:: additional processing before/after sampler{br}
  -> (for preparation of request / analysis of response)
image:jmeter-images/question.gif[height=28] Assertions:: describe success conditions
image:jmeter-images/meter.png[height=28] Listeners:: collect the metrics


JMeter - Tree
-------------

[style="asciidoc",cols="<2,<1",grid="none",frame="none"]
|=====
|All element are arranged as an ordered tree.

* *Thread Groups*, *Logic Controller* and *Samplers*{br}
  -> form the execution flow, hence their order is important.
* *Configuration Elements*, *Timers*, *Post/Pre-processors*, *Assertions*, *Listeners* {br}
  -> replicate their behavior to all elements in their scope (i.e. in the subtree delimited by their parent)

|
image::sample_jmeter_tree.png[width="75%"]
|=====


JMeter - Lifecycle
------------------

[graphviz]
-----
digraph G {
  size = "9,9";
  splines="line";
  node[shape=box];
  edge[weight=2];

  subgraph cluster_0 {
    label = "Test Plan"

    begin0 -> setup -> test0
    p_seq -> test0:w [dir=back taillabel="sequential" arrowtail=none]
    test0:e -> p_par [headlabel="parallel" arrowhead=none]
    p_seq -> main_seq
    p_par -> main_par
    { rank=same; p_seq; test0; p_par }
    main_seq -> teardown
    main_par -> teardown
    { rank=same; main_seq; main_par }
    teardown -> done0
    setup[shape=record,label="{setUp Thread Group 1|...|setUp Thread Group N}"]
    main_seq[shape=record label="{Thread Group 1|...|Thread Group N}" style=filled fillcolor=palegreen]
    main_par[shape=record label="Thread\nGroup 1|...|Thread\nGroup N" style=filled fillcolor=palegreen]
    teardown[shape=record label="{tearDown Thread Group 1|...|tearDown Thread Group N}"]

    test0[label="?", shape=diamond]
    begin0 [label="" shape=circle]
    done0 [label="" shape=circle style=filled]
    p_seq [width=0 shape=point label=""]
    p_par [width=0 shape=point label=""]
  }

  subgraph cluster_1 {
    label = "Thread"
    style = filled
    fillcolor = palegreen

    begin1 -> p1_2 [arrowhead=none]
    p1_2 -> steps -> test1
    steps[shape=record label="{<f1> Step 1|...|Step n}" style=filled fillcolor=lightpink]
    p1_2 -> p1_1 [weight=1 dir=back]
    p1_0 -> p1_1 [dir=back arrowtail=none]
    test1:e -> p1_0 [weight=1 label="next iteration" arrowhead=none]
    { rank=same; test1; p1_0 }
    { rank=same; p1_1; p1_2 }
    test1 -> done1 [label="thread completed"]

    test1[label="?", shape=diamond]
    begin1[label="",shape=circle]
    done1[label="" shape=circle style=filled]
    p1_0 [width=0 shape=point label=""]
    p1_1 [width=0 shape=point label=""]
    p1_2 [width=0 shape=point label=""]
  }

  subgraph cluster_2 {
    label = "1 step"
    style = filled
    fillcolor = lightpink

    begin2 -> "Configuration elements" -> "Pre-Processors" -> Timers -> Sampler

    Sampler [penwidth=2 fontsize=20 style="bold filled" style=filled fillcolor=lightblue]
    test2:e -> p2_0 [weight=1 headlabel="SampleResult\nis null" arrowhead=none]
    { rank=same; test2; p2_0 }
    p2_0 -> p2_1 [arrowhead=none]
    p2_1 -> done2 [weight=1]
    { rank=same; done2; p2_1 }
    test2 -> "Post-Processors" [label=" no"]
    "Post-Processors" -> Assertions -> Listeners -> done2

    Sampler -> test2

    test2[label="?" shape=diamond]
    begin2[label="",shape=circle]
    done2[label="" shape=circle style=filled]
    p2_0 [width=0 shape=point label=""]
    p2_1 [width=0 shape=point label=""]
    p2_2 [width=0 shape=point label=""]
    p2_3 [width=0 shape=point label=""]
  }
}
-----


JMeter - Controllers
--------------------

[horizontal]
Once Only Controller:: executed only at the first iteration
Loop Controller:: repeat n times (or forever) -> iterations
If Controller:: conditional
Switch Controller:: alternative


NOTE: each Thread Group is an implicit Loop Controller at the same time


JMeter - Configuration elements
-------------------------------

[horizontal]
User Defined Variables:: Defines a set of variables with fixed values.
CSV Data Set Config:: Defines variables that will change at each _iteration_.
HTTP Request Defaults:: predefine parts of the request, like host, port, protocol...
HTTP Cookie Manager:: automatic management of cookies


JMeter - Timers
---------------

[horizontal]
Random Timers:: simulate end-user "think time"
Constant Throughput Timer:: "flat" throughput (but setpoint maybe changed at any time during execution...)
jp{nil}@{nil}gc - Throughput Shaping Timer:: variable throughput
BeanShell Timer:: explicit computing of the time

[WARNING]
=====
Throughput timers may not work well in extreme conditions (very high or very low throughput).

-> *BeanShell Timer* with your own logic
=====


JMeter - Samplers
-----------------

[horizontal]
HTTP Request:: most often used
BeanShell Sampler:: e.g. when you need to use a specific SOAP client (cf. SOAP security)
JDBC Request:: calling a DB
Debug Sampler:: generate a "dummy" sample with all variables and properties values, very usefull for debugging


JMeter - Assertions
-------------------

* Listeners will collect information about each sampler execution
* Each listener is also able to ouput results to a file (CSV or XML format)
* When generating statistics, listeners will group samples that have the same "label"
  (independantly from the originating sampler)

[horizontal]
Response Assertion:: verify any part of the HTTP response, including the body
XPath Assertion:: sometimes more precise, especially for SOAP
BeanShell Assertion:: for more complex verifications


JMeter - Listeners
------------------

_____
[horizontal]
Aggregate Report:: give a few statistics on each sampler -> ideal for saving the results (enter a filename)
View Results Tree:: all request and all responses, very usefull for debugging, but huge memory impact => always remember to disable!
jp{nil}@{nil}gc - Active Threads Over Time:: threads count over time, usefull for debugging to verify that the load profile is correct
jp{nil}@{nil}gc - Transactions per Second:: throuhgput over time, usefull for debugging to verify that the load profile is correct
_____


* In GUI mode, Listeners are able to re-read a saved results file and re-draw / re-calculte the stats


JMeter - Variables & Properties
-------------------------------

*Variables*

* Each thread gets its own set of variables.
* A variable usually contains a String, but may contain any kind of Object.

*Properties*

* Shared by all threads.
* Set from the command line: +-Jproperty_name=property_value+
* Copy in variable: +$\{__P(property_name, default_value)}+
* A property usually contains a String, but may contain any kind of Object.
* The "Non-Test Element" *Property Display* may be added to the *WorkBench* special node as a handy way of checking propertie values


JMeter expressions & functions
------------------------------

*Expressions*

[horizontal]
value of a variable:: +$\{variable_name}+
function call:: +$\{__function_name(att1, att2, ..., attn)}+
implicit concatenation:: e.g. +file_$\{var}.txt+

*Functions*

[horizontal]
property value:: +$\{__P(property_name, default_value)}+
thread number:: +$\{__threadNum}+
uuid:: +$\{__UUID}+
timestamp:: +$\{__time(YMDHMS)}+


BeanShell
---------

* Accepts Java-like syntax _(limited to features present in version 4 of Java syntax!)_
* Has acces to any Java classes, methods, and members
* Very powerful but may be hard to debug


Randomization
-------------

* Data randomization may be a CPU intensive processing
  => randomize your data before test, outside of jmeter (e.g. shell scripting, Excel...)
* Randomize both values and order of parameters
* *Pragmatism* will tell you not to try randomizing parameters that are irrelevant (i.e. never used as a key for cache nor for index){br}
  *Experience* will tell you to never trust a pragmatic developer who tells you a parameter is irrelevant{br}
  => be *smart*
* Avoid using contant timers, prefer randomized ones, to avoid repetitiive cycle effects
* In case you know that the UoT is using some caching mechanism, make sure your sample data
  is such that the cache hit/miss rates correspond to reality (may be quite difficult to assess!)
