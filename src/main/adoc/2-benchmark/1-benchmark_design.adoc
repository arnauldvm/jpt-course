// build_options: 
Java Performance Tuning - Designing your benchmarks
===================================================
Arnauld Van Muysewinkel <avm@pendragon.be>
v0.0, dd-mmm-2015: Draft version
:backend: slidy
//:theme: volnitsky
:data-uri:
:copyright: Creative-Commons-Zero (Arnauld Van Muysewinkel)
:icons:
:br: pass:[<br>]

_(link:../0-extra/1-training_plan.html#(5)[back to plan])_

Content
-------

* introduction
** what is a performance test, objective (p4,5)

* planning your tests
** Unit of Test (p6)
* load profiles
** Types of performance tests (p12-13)
* metrics
** Performance metrics (p8-9)
** work organization (documents) (p17)

* anatomy of a test


Content (bis)
-------------


* Test the test (p7)
* Possible results (p15)
* Steps (p16)
* actors (p20)
* test env (vs. UoT) (p21)
* usage patterns - time (p22-26)
* usage patterns - data (caching!) (p27-28)
* test execution (p29-31)
* analyse/verify results (p32-35)


Introduction
------------

====
* References
* What is a performance test?
====

-> Reference
------------

Performance tuning::
* *Java Performance Tuning (training/workshop). Kirk Perdine. Kodewerk*
   http://www.kodewerk.com/workshop.html


-> What is a performance test?
------------------------------

Test pursuing one or several of the following goals:

* assess
** responsiveness
** throughput
** concurrency
** scalability
** stability
* => measure the metrics and compare with expected values
* => in case of poor performance: identify the origin


Planning your test
------------------

====
* What are we testing?
//* Why are we testing?
* Types of load profiles
* Performance metrics
* Documentation
====


-> What are we testing?
----------------------

[WARNING]
====
Define carefully the limits of the system being studied.

*"Unit of Test" &ne; "Test Harness"*
====

"Unit of Test" (UoT) = System being studied

{br}

"Test Harness" = the supporting infrastructure

* = system infrastructure + testing infrastructure
* system infrastructure must be iso-PROD!{br}
  (sizing, configuration... _all pieces of hardware_!)

-> -> Examples ...


Simple example of Test Harness
------------------------------

Let UoT = a single webapp, materialized by a WAR deployable,{br}
in a traditional infrastructure (i.e. virtualized, not containerized)

System infrastructure:

* RP cluster (2 nodes)
** 2 Linux virtual servers
* Weblogic cluster (2 sync nodes, 2 JVM's)
** 2 Linux virtual servers
* Weblogic admin
** 1 Linux virtual server (1 JVM)
* Database cluster (RAC)
** 3 Linux virtual servers


Simple example of Test Harness (cont'd)
---------------------------------------

But also:

* Hypervisors (ESX, or OVM, or ...)
** Physical servers
* VLAN
** Routers
** Firewalls
* Storage units
* Monitoring?


Simple example of Test Harness (cont'd)
---------------------------------------

Testing infrastructure:

* Injector
** Physical server
** VLAN
** Firewall?

! Must be distinct from the system infrastructure!{br}
Make sure your measure does not modify the performance of the system being tested.

More complex example of Test Harness
------------------------------------

Let UoT = a more complex (and more realistic!) JEE system:

* sync deployable (EAR)
* async deployable (EAR)
* invoking several SOAP WS
* sending/receiving messages through a messaging system (JMS, MQ...)
* ...

-> Where do we put the limits of the system?{br}
-> Are we able to duplicate the world?{br}
-> How do we make our test measures independent of the dependencies behaviour?


-> Types of Load profiles
-------------------------

load test::
** expected workload: the load is _under control_, in term of: # concurrent users +and+ throughput
** focus: system meeting requirements
stress test::
** high workloads, saturation
** focus: throughput and stability
** ! impact on the rest of the infrastructure (network, firewall, RP...)
** ! abnormal conditions, do not try to (over-)tune the response time
** -> look for failures


Types of Load profiles (cont'd)
-------------------------------

spike test::
** idem stress test with more saturation and shorter duration
** short term 
endurance test::
** long period of time
** focus: stability of the response time
reference test::
* one user
* no delay between steps
* many loops


-> Performance metrics
----------------------

Measures of the UoT (&ne; measures/monitoring of the Test Harness)

[horizontal]
Response time:: (time response is received) - (time request was sent){br}
  _! the timestamp given in the logs may be either the time of request or the time of response,
  depending on the system_
Throughput:: &Delta;n/&Delta;t; where n = number of transactions
Concurrency:: count of concurrent requests at a given time{br}
  _(difficult to compute precisely: sort all timestamps (request *and* response), then travel the list while incrementing (request) / decrementing (response) a counter_


Performance metrics (cont'd)
----------------------------

Cumulated time:: &Sigma; response-time / &Delta;t
Workload:: &Delta;work/&Delta;t; where work is a measure of the work accomplished (e.g. # records, file size...)
Capacity:: how much workload the system can absorb
Availability:: amount of time system is available / &Delta;t
Scalability:: ability of the system to utilize more (or less) hardware to match variations of the workload (!up &ne; down)


Performance metrics statistics
------------------------------

f(t) (e.g. response time, concurrency)

* Average
* Min, Max
* Percentile

f(t, sampling) (calculated, e.g. throughput, cumulated time)

* overall: sampling period = whole test duration
* idem f(t)


Measure window
--------------

A scalar measure (e.g. average response time) should be taken only
over a time window where the system is in a steady state
(i.e. metrics remain stable), typically _after_ the rampup.


-> Documentation
----------------

* Architecture document of the UoT
* Test plan:
** goals
** scope (boundaries of the UoT)
** resources required (including key people!)
** schedulling
* Requirements: performance goals
* Test protocol: process, scenarios, load profiles...
* Test report:
** all results (measures and calculations)
** conclusions


